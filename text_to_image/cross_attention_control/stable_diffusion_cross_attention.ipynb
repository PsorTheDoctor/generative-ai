{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNFUzbmYTXb446hOLm37h2b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7c03e0d9cec0426d8b3dec79597eac30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2bc726d2c8a4a3e8093b3bb06991eba",
              "IPY_MODEL_9a4c05f8a9174a7a894b2b3d65eb66f1",
              "IPY_MODEL_2165cd488dc2463786afc38e8ef712b0",
              "IPY_MODEL_9fb9a154eae8407092bf7c6228bd4b6e"
            ],
            "layout": "IPY_MODEL_07e4aa72cb9d4ed08b64ffad0cf1e43a"
          }
        },
        "e2bc726d2c8a4a3e8093b3bb06991eba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2e5f39294f84f01a70eb2c35f214fe1",
            "placeholder": "​",
            "style": "IPY_MODEL_949477724b8d46659f146e25cdec6669",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "9a4c05f8a9174a7a894b2b3d65eb66f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_93b47177141e41f28cf09aff8090dd51",
            "placeholder": "​",
            "style": "IPY_MODEL_30dcb8cbe54d4a0f8fda1fa07ea4d4bc",
            "value": ""
          }
        },
        "2165cd488dc2463786afc38e8ef712b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_443da7cb60e94e42a97cc63693617b8d",
            "style": "IPY_MODEL_b99c2cf227cb47beb1a799cdeeefda01",
            "tooltip": ""
          }
        },
        "9fb9a154eae8407092bf7c6228bd4b6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd38e31fcf904b0199e90dae0ad16e7d",
            "placeholder": "​",
            "style": "IPY_MODEL_cf3bf16269564a41a5ef93bbd459f0b7",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "07e4aa72cb9d4ed08b64ffad0cf1e43a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "a2e5f39294f84f01a70eb2c35f214fe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "949477724b8d46659f146e25cdec6669": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93b47177141e41f28cf09aff8090dd51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30dcb8cbe54d4a0f8fda1fa07ea4d4bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "443da7cb60e94e42a97cc63693617b8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b99c2cf227cb47beb1a799cdeeefda01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "cd38e31fcf904b0199e90dae0ad16e7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf3bf16269564a41a5ef93bbd459f0b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PsorTheDoctor/artificial-intelligence/blob/master/modern_approach/text_to_image/cross_attention_control/stable_diffusion_cross_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stable Diffusion: Cross Attention Control\n",
        "\n",
        "![img](https://github.com/PsorTheDoctor/artificial-intelligence/blob/master/modern_approach/text_to_image/cross_attn.png?raw=true)"
      ],
      "metadata": {
        "id": "QkUOW3OorXaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade diffusers==0.3.0 transformers scipy"
      ],
      "metadata": {
        "id": "8MYIMBpKtwr9"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "gsUqSX9bpiV8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "from diffusers import LMSDiscreteScheduler\n",
        "from tqdm.auto import tqdm\n",
        "from torch import autocast\n",
        "from difflib import SequenceMatcher\n",
        "import torch\n",
        "from transformers import CLIPModel, CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299,
          "referenced_widgets": [
            "7c03e0d9cec0426d8b3dec79597eac30",
            "e2bc726d2c8a4a3e8093b3bb06991eba",
            "9a4c05f8a9174a7a894b2b3d65eb66f1",
            "2165cd488dc2463786afc38e8ef712b0",
            "9fb9a154eae8407092bf7c6228bd4b6e",
            "07e4aa72cb9d4ed08b64ffad0cf1e43a",
            "a2e5f39294f84f01a70eb2c35f214fe1",
            "949477724b8d46659f146e25cdec6669",
            "93b47177141e41f28cf09aff8090dd51",
            "30dcb8cbe54d4a0f8fda1fa07ea4d4bc",
            "443da7cb60e94e42a97cc63693617b8d",
            "b99c2cf227cb47beb1a799cdeeefda01",
            "cd38e31fcf904b0199e90dae0ad16e7d",
            "cf3bf16269564a41a5ef93bbd459f0b7"
          ]
        },
        "id": "ZBmpvWgzu38E",
        "outputId": "db6c2791-42f6-4ba7-e298-ccf5ea4bca20"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Login successful\n",
            "Your token has been saved to /root/.huggingface/token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path_clip = \"openai/clip-vit-large-patch14\"\n",
        "clip_tokenizer = CLIPTokenizer.from_pretrained(model_path_clip)\n",
        "clip_model = CLIPModel.from_pretrained(model_path_clip, torch_dtype=torch.float16)\n",
        "clip = clip_model.text_model\n",
        "model_path_diffusion = \"CompVis/stable-diffusion-v1-4\"\n",
        "\n",
        "unet = UNet2DConditionModel.from_pretrained(\n",
        "    model_path_diffusion, subfolder=\"unet\", use_auth_token=True, revision=\"fp16\", torch_dtype=torch.float16\n",
        ")\n",
        "vae = AutoencoderKL.from_pretrained(\n",
        "    model_path_diffusion, subfolder=\"vae\", use_auth_token=True, revision=\"fp16\", torch_dtype=torch.float16\n",
        ")\n",
        "device = \"cuda\"\n",
        "unet.to(device)\n",
        "vae.to(device)\n",
        "clip.to(device)"
      ],
      "metadata": {
        "id": "Xuyv64XlqC1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_attention_weights(weight_tuples):\n",
        "    tokens_length = clip_tokenizer.model_max_length\n",
        "    weights = torch.ones(tokens_length)\n",
        "    \n",
        "    for i, w in weight_tuples:\n",
        "        if i < tokens_length and i >= 0:\n",
        "            weights[i] = w\n",
        "    \n",
        "    for name, module in unet.named_modules():\n",
        "        module_name = type(module).__name__\n",
        "        if module_name == \"CrossAttention\" and \"attn2\" in name:\n",
        "            module.last_attn_slice_weights = weights.to(device)\n",
        "        if module_name == \"CrossAttention\" and \"attn1\" in name:\n",
        "            module.last_attn_slice_weights = None\n",
        "    \n",
        "def init_attention_edit(tokens, tokens_edit):\n",
        "    tokens_length = clip_tokenizer.model_max_length\n",
        "    mask = torch.zeros(tokens_length)\n",
        "    indices_target = torch.arange(tokens_length, dtype=torch.long)\n",
        "    indices = torch.zeros(tokens_length, dtype=torch.long)\n",
        "    tokens = tokens.input_ids.numpy()[0]\n",
        "    tokens_edit = tokens_edit.input_ids.numpy()[0]\n",
        "    \n",
        "    for name, a0, a1, b0, b1 in SequenceMatcher(None, tokens, tokens_edit).get_opcodes():\n",
        "        if b0 < tokens_length:\n",
        "            if name == \"equal\" or (name == \"replace\" and a1-a0 == b1-b0):\n",
        "                mask[b0:b1] = 1\n",
        "                indices[b0:b1] = indices_target[a0:a1]\n",
        "\n",
        "    for name, module in unet.named_modules():\n",
        "        module_name = type(module).__name__\n",
        "        if module_name == \"CrossAttention\" and \"attn2\" in name:\n",
        "            module.last_attn_slice_mask = mask.to(device)\n",
        "            module.last_attn_slice_indices = indices.to(device)\n",
        "        if module_name == \"CrossAttention\" and \"attn1\" in name:\n",
        "            module.last_attn_slice_mask = None\n",
        "            module.last_attn_slice_indices = None\n",
        "\n",
        "def init_attention_func():\n",
        "    def new_attention(self, query, key, value, sequence_length, dim):\n",
        "        batch_size_attention = query.shape[0]\n",
        "        hidden_states = torch.zeros(\n",
        "            (batch_size_attention, sequence_length, dim // self.heads), device=query.device, dtype=query.dtype\n",
        "        )\n",
        "        slice_size = self._slice_size if self._slice_size is not None else hidden_states.shape[0]\n",
        "        for i in range(hidden_states.shape[0] // slice_size):\n",
        "            start_idx = i * slice_size\n",
        "            end_idx = (i + 1) * slice_size\n",
        "            attn_slice = (\n",
        "                torch.einsum(\"b i d, b j d -> b i j\", query[start_idx:end_idx], key[start_idx:end_idx]) * self.scale\n",
        "            )\n",
        "            attn_slice = attn_slice.softmax(dim=-1)\n",
        "            \n",
        "            if self.use_last_attn_slice:\n",
        "                if self.last_attn_slice_mask is not None:\n",
        "                    new_attn_slice = torch.index_select(self.last_attn_slice, -1, self.last_attn_slice_indices)\n",
        "                    attn_slice = attn_slice * (1 - self.last_attn_slice_mask) + new_attn_slice * self.last_attn_slice_mask\n",
        "                else:\n",
        "                    attn_slice = self.last_attn_slice\n",
        "                \n",
        "                self.use_last_attn_slice = False\n",
        "                    \n",
        "            if self.save_last_attn_slice:\n",
        "                self.last_attn_slice = attn_slice\n",
        "                self.save_last_attn_slice = False\n",
        "                \n",
        "            if self.use_last_attn_weights and self.last_attn_slice_weights is not None:\n",
        "                attn_slice = attn_slice * self.last_attn_slice_weights\n",
        "                self.use_last_attn_weights = False\n",
        "\n",
        "            attn_slice = torch.einsum(\"b i j, b j d -> b i d\", attn_slice, value[start_idx:end_idx])\n",
        "            hidden_states[start_idx:end_idx] = attn_slice\n",
        "\n",
        "        # reshape hidden_states\n",
        "        hidden_states = self.reshape_batch_dim_to_heads(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "    for name, module in unet.named_modules():\n",
        "        module_name = type(module).__name__\n",
        "        if module_name == \"CrossAttention\":\n",
        "            module.last_attn_slice = None\n",
        "            module.use_last_attn_slice = False\n",
        "            module.use_last_attn_weights = False\n",
        "            module.save_last_attn_slice = False\n",
        "            module._attention = new_attention.__get__(module, type(module))\n",
        "            \n",
        "def use_last_tokens_attention(use=True):\n",
        "    for name, module in unet.named_modules():\n",
        "        module_name = type(module).__name__\n",
        "        if module_name == \"CrossAttention\" and \"attn2\" in name:\n",
        "            module.use_last_attn_slice = use\n",
        "            \n",
        "def use_last_tokens_attention_weights(use=True):\n",
        "    for name, module in unet.named_modules():\n",
        "        module_name = type(module).__name__\n",
        "        if module_name == \"CrossAttention\" and \"attn2\" in name:\n",
        "            module.use_last_attn_weights = use\n",
        "            \n",
        "def use_last_self_attention(use=True):\n",
        "    for name, module in unet.named_modules():\n",
        "        module_name = type(module).__name__\n",
        "        if module_name == \"CrossAttention\" and \"attn1\" in name:\n",
        "            module.use_last_attn_slice = use\n",
        "            \n",
        "def save_last_tokens_attention(save=True):\n",
        "    for name, module in unet.named_modules():\n",
        "        module_name = type(module).__name__\n",
        "        if module_name == \"CrossAttention\" and \"attn2\" in name:\n",
        "            module.save_last_attn_slice = save\n",
        "            \n",
        "def save_last_self_attention(save=True):\n",
        "    for name, module in unet.named_modules():\n",
        "        module_name = type(module).__name__\n",
        "        if module_name == \"CrossAttention\" and \"attn1\" in name:\n",
        "            module.save_last_attn_slice = save\n",
        "            \n",
        "@torch.no_grad()\n",
        "def generate_sample(prompt='', prompt_edit=None, prompt_edit_token_weights=[], \n",
        "                    prompt_edit_tokens_start=0.0, prompt_edit_tokens_end=1.0, \n",
        "                    prompt_edit_spatial_start=0.0, prompt_edit_spatial_end=1.0, \n",
        "                    guidance_scale=7.5, steps=50, seed=None, width=512, height=512, \n",
        "                    init_image=None, init_image_strength=0.5):\n",
        "  \n",
        "    #Change size to multiple of 64 to prevent size mismatches inside model\n",
        "    width = width - width % 64\n",
        "    height = height - height % 64\n",
        "    \n",
        "    #If seed is None, randomly select seed from 0 to 2^32-1\n",
        "    if seed is None: seed = random.randrange(2**32 - 1)\n",
        "    generator = torch.cuda.manual_seed(seed)\n",
        "    \n",
        "    #Set inference timesteps to scheduler\n",
        "    scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, \n",
        "                                     beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "    scheduler.set_timesteps(steps)\n",
        "    \n",
        "    #Preprocess image if it exists (img2img)\n",
        "    if init_image is not None:\n",
        "        #Resize and transpose for numpy b h w c -> torch b c h w\n",
        "        init_image = init_image.resize((width, height), resample=Image.Resampling.LANCZOS)\n",
        "        init_image = np.array(init_image).astype(np.float32) / 255.0 * 2.0 - 1.0\n",
        "        init_image = torch.from_numpy(init_image[np.newaxis, ...].transpose(0, 3, 1, 2))\n",
        "        \n",
        "        #If there is alpha channel, composite alpha for white, as the diffusion model does not support alpha channel\n",
        "        if init_image.shape[1] > 3:\n",
        "            init_image = init_image[:, :3] * init_image[:, 3:] + (1 - init_image[:, 3:])\n",
        "            \n",
        "        #Move image to GPU\n",
        "        init_image = init_image.to(device)\n",
        "        \n",
        "        #Encode image\n",
        "        with autocast(device):\n",
        "            init_latent = vae.encode(init_image).latent_dist.sample(generator=generator) * 0.18215\n",
        "            \n",
        "        t_start = steps - int(steps * init_image_strength)\n",
        "    else:\n",
        "        init_latent = torch.zeros((1, unet.in_channels, height//8, width//8), device=device)\n",
        "        t_start = 0\n",
        "    \n",
        "    #Generate random normal noise\n",
        "    noise = torch.randn(init_latent.shape, generator=generator, device=device)\n",
        "    latent = scheduler.add_noise(init_latent, noise, t_start).to(device)\n",
        "    \n",
        "    #Process clip\n",
        "    with autocast(device):\n",
        "        tokens_unconditional = clip_tokenizer(\"\", padding=\"max_length\", max_length=clip_tokenizer.model_max_length, \n",
        "                                              truncation=True, return_tensors=\"pt\", return_overflowing_tokens=True)\n",
        "        embedding_unconditional = clip(tokens_unconditional.input_ids.to(device)).last_hidden_state\n",
        "\n",
        "        tokens_conditional = clip_tokenizer(prompt, padding=\"max_length\", max_length=clip_tokenizer.model_max_length, \n",
        "                                            truncation=True, return_tensors=\"pt\", return_overflowing_tokens=True)\n",
        "        embedding_conditional = clip(tokens_conditional.input_ids.to(device)).last_hidden_state\n",
        "\n",
        "        #Process prompt editing\n",
        "        if prompt_edit is not None:\n",
        "            tokens_conditional_edit = clip_tokenizer(prompt_edit, padding=\"max_length\", max_length=clip_tokenizer.model_max_length, \n",
        "                                                     truncation=True, return_tensors=\"pt\", return_overflowing_tokens=True)\n",
        "            embedding_conditional_edit = clip(tokens_conditional_edit.input_ids.to(device)).last_hidden_state\n",
        "            \n",
        "            init_attention_edit(tokens_conditional, tokens_conditional_edit)\n",
        "            \n",
        "        init_attention_func()\n",
        "        init_attention_weights(prompt_edit_token_weights)\n",
        "        timesteps = scheduler.timesteps[t_start:]\n",
        "        \n",
        "        for i, t in tqdm(enumerate(timesteps), total=len(timesteps)):\n",
        "            t_index = t_start + i\n",
        "\n",
        "            sigma = scheduler.sigmas[t_index]\n",
        "            latent_model_input = latent\n",
        "            latent_model_input = (latent_model_input / ((sigma**2+1) ** 0.5)).to(unet.dtype)\n",
        "\n",
        "            #Predict the unconditional noise residual\n",
        "            noise_pred_uncond = unet(latent_model_input, t, encoder_hidden_states=embedding_unconditional).sample\n",
        "            \n",
        "            #Prepare the Cross-Attention layers\n",
        "            if prompt_edit is not None:\n",
        "                save_last_tokens_attention()\n",
        "                save_last_self_attention()\n",
        "            else:\n",
        "                #Use weights on non-edited prompt when edit is None\n",
        "                use_last_tokens_attention_weights()\n",
        "                \n",
        "            #Predict the conditional noise residual and save the cross-attention layer activations\n",
        "            noise_pred_cond = unet(latent_model_input, t, encoder_hidden_states=embedding_conditional).sample\n",
        "            \n",
        "            #Edit the Cross-Attention layer activations\n",
        "            if prompt_edit is not None:\n",
        "                t_scale = t / scheduler.num_train_timesteps\n",
        "                if t_scale >= prompt_edit_tokens_start and t_scale <= prompt_edit_tokens_end:\n",
        "                    use_last_tokens_attention()\n",
        "                if t_scale >= prompt_edit_spatial_start and t_scale <= prompt_edit_spatial_end:\n",
        "                    use_last_self_attention()\n",
        "                    \n",
        "                #Use weights on edited prompt\n",
        "                use_last_tokens_attention_weights()\n",
        "\n",
        "                #Predict the edited conditional noise residual using the cross-attention masks\n",
        "                noise_pred_cond = unet(latent_model_input, t, encoder_hidden_states=embedding_conditional_edit).sample\n",
        "                \n",
        "            #Perform guidance\n",
        "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
        "\n",
        "            latent = scheduler.step(noise_pred, t_index, latent).prev_sample\n",
        "\n",
        "        #scale and decode the image latents with vae\n",
        "        latent = latent / 0.18215\n",
        "        image = vae.decode(latent.to(vae.dtype)).sample\n",
        "\n",
        "    image = (image / 2 + 0.5).clamp(0, 1)\n",
        "    image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "    image = (image[0] * 255).round().astype(\"uint8\")\n",
        "    return Image.fromarray(image)\n",
        "\n",
        "def prompt_token(prompt, index):\n",
        "    tokens = clip_tokenizer(prompt, padding=\"max_length\", max_length=clip_tokenizer.model_max_length, \n",
        "                            truncation=True, return_tensors=\"pt\", return_overflowing_tokens=True).input_ids[0]\n",
        "    return clip_tokenizer.decode(tokens[index : index+1])"
      ],
      "metadata": {
        "id": "YMhHD8ctqRrE"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_token('an apple cake', 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "DUVA-PWNV_jM",
        "outputId": "90b4b796-7a0a-4e71-c3ed-9458b943c20d"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'apple'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sample('an apple cake', seed=44)"
      ],
      "metadata": {
        "id": "eYFGaft0V6OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sample('an apple cake', 'a cheese cake', seed=44)"
      ],
      "metadata": {
        "id": "nHz4PH_lSnFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sample('an apple cake', 'a chocolate cake', seed=44)"
      ],
      "metadata": {
        "id": "7i5igQX4VOH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sample('an apple cake', 'an orange cake', seed=44)"
      ],
      "metadata": {
        "id": "hAp1MZlwVf7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sample('an apple cake', 'a jelly cake', seed=44)"
      ],
      "metadata": {
        "id": "8htL4ijyWqWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sample('an apple cake', 'a marzipan cake', seed=44)"
      ],
      "metadata": {
        "id": "peZlxjd_Y2st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sample('an apple cake', 'a coffee cake', seed=44)"
      ],
      "metadata": {
        "id": "P2Ii2hXiZqxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sample('an apple cake', 'an ice-cream cake', seed=44)"
      ],
      "metadata": {
        "id": "X70tPC39ZLG6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
